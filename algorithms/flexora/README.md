## Repo
```
git clone https://github.com/trng28/peft-lora-quantization
cd peft-lora-quantization/algorithms/flexora
```
## Installation
```
pip install -r requirements.txt
```

## Search Layer for fine-tuning stage
```
python main.py
```

## Finetune
```
python finetune.py
```

## Reference
```bib
@inproceedings{wei2025flexora,
  title={Flexora: Flexible low-rank adaptation for large language models},
  author={Wei, Chenxing and Shu, Yao and He, Ying Tiffany and Yu, Fei},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={14643--14682},
  year={2025}
}
```