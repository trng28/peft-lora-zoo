# PEFT, LoRA, Quantization Techniques Collection


| Paper                                                                                                            | Venue    |
| ---------------------------------------------------------------------------------------------------------------- | -------- |
|[**SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models**](https://openreview.net/forum?id=PO9bBEPNWy)|ICML'25|
|[**MLWQ: Efficient Small Language Model Deployment via Multi-Level Weight Quantization**](https://aclanthology.org/2025.emnlp-main.408.pdf)|EMNLP'25|
|[**L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models**](https://aclanthology.org/2025.acl-long.99.pdf)|ACL'25|
|[**Flexora: Flexible Low-Rank Adaptation for Large Language Models**](https://aclanthology.org/2025.acl-long.713.pdf)|ACL'25|
| [**DenseLoRA: Dense Low-Rank Adaptation of Large Language Models**](https://aclanthology.org/2025.acl-long.503/) | ACL'25 |
| [**BeamLoRA: Beam-Constraint Low-Rank Adaptation**](https://aclanthology.org/2025.acl-long.582.pdf) | ACL'25 |
|[**LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices**](https://aclanthology.org/2025.naacl-long.393.pdf)|NAACL'25|
|[**MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning**](https://aclanthology.org/2025.naacl-long.248.pdf)| NAACL'25|
|[**SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model**](https://aclanthology.org/2025.naacl-long.230.pdf)|NAACL'25|
|[**META-LORA: Memory-Efficient Sample Reweighting for Fine-Tuning Large Language Models**](https://aclanthology.org/2025.coling-main.568.pdf)| COLING'25|
|[**Exploring Quantization for Efficient Pre-Training of Transformer Language Models**](https://aclanthology.org/2024.findings-emnlp.787.pdf)|EMNLP'24|
|[**ApiQ: Finetuning of 2-Bit Quantized Large Language Model**](https://aclanthology.org/2024.emnlp-main.1168.pdf)|EMNLP'24|
|[**On the Impact of Calibration Data in Post-training Quantization and Pruning**](https://aclanthology.org/2024.acl-long.544.pdf)|ACL'24|
|[**LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models**](https://aclanthology.org/2024.acl-long.122.pdf)|ACL'24|
|[**DoRA: Weight-Decomposed Low-Rank Adaptation**](https://openreview.net/pdf?id=3d5CIRG1n2)|ICML'24|
|[**OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models**](https://openreview.net/pdf?id=8Wuvhh0LYW)|ICLR'24|
|[**Pissa: Principal singular values and singular vectors adaptation of large language models**](https://proceedings.neurips.cc/paper_files/paper/2024/file/db36f4d603cc9e3a2a5e10b93e6428f2-Paper-Conference.pdf)| NeurIPS'24|
| [**QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning**](https://aclanthology.org/2024.emnlp-industry.53.pdf) | EMNLP'24 |
| [**AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning**](https://aclanthology.org/2024.naacl-long.282.pdf) | NAACL'24 |
| [**ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models**](https://aclanthology.org/2024.naacl-long.35.pdf) | NAACL'24 |
|[**SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models**](https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf)|ICML'23|
| [**AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning**](https://arxiv.org/pdf/2303.10512) | ICLR'23 |
| [**DyLoRA: Parameter-efficient tuning of pre-trained models using dynamic search-free low-rank adaptation**](https://aclanthology.org/2023.eacl-main.239.pdf) | EACL'23 |
|[**Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization**](https://aclanthology.org/2023.emnlp-main.910.pdf)|EMNLP'23|
|[**OPTQ: Accurate Quantization for Generative Pre-trained Transformers**](https://openreview.net/pdf?id=tcbBPnfwxS)|ICLR'23|
|[**QuIP: 2-Bit Quantization of Large Language Models With Guarantees**](https://proceedings.neurips.cc/paper_files/paper/2023/file/0df38cd13520747e1e64e5b123a78ef8-Paper-Conference.pdf)| NeurIPS'23|
|[**Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization**](https://proceedings.neurips.cc/paper_files/paper/2023/file/7183f4fc87598f6c6e947b96714acbd6-Paper-Conference.pdf)|NeurIPS'23|
| [**QLoRA: Efficient Finetuning of Quantized LLMs**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html) | NeurIPS'23 |
---